---
layout: post
comments: true
title : Decision Tree
categories: [Machine Learning]
---

## Decision Tree (의사결정나무)

- 목적
  - 한번에 하나의 설명변수를 사용하여 정확한 규칙(특정 기준점 생성)들을 생성

- 규칙 예시
  - 만일 내일 날씨가 맑고 습도가 70% 이하이면 운동경기가 열릴 것이다. or
  - 만일 내일 비가 오고 바람이 불면 운동경기가 열리지 않을 것이다.

### 좋은 기준점을 어떻게 정할까
  - 좋은 기준점이란? 해당 기준점으로 나누었을때 각 영역의 복잡도가 낮은 기준점을 좋은 기준점이라고 한다.

  - 좋은 기준점을 찾는 방법론
    1. 각 변수를 이용해 가능한 모든 기준점을 통해 구간을 분리한다.
    2. 분리된 구간의 복잡도가 가장 낮은 기준점을 선택한다.
    3. 위 의 과정을 계속 반복한다.
      * 무수히 많은 기준점 중 엔트로피가 가장 낮아지는 것을 선택합니다.
      * 엔트로피(복잡도를 수학적으로 표현한 식)

### 오버피팅이란
  - 학습데이터를 너무 과도하게 학습하는것을 뜻한다.
  - 학습데이터를 과도하게 학습하면, outlier에 민감하다.

- 오버피팅 방지
  - 분류를 했을 때 각 구역에서 객체의 개수가 n개 이상일 때만 분류하기

### cross-validation
  - 프로세스
    1. 1번 데이터셋을 test데이터, 2번, 3번을 train 데이터로 두어 1번데이터에서의 정확도계산
    2. 2번 데이터셋을 test데이터, 1번, 3번을 train 데이터로 두어 2번데이터에서의 정확도계산
    3. 3번 데이터셋을 test데이터, 1번, 2번을 train 데이터로 두어 3번데이터에서의 정확도계산
    4. 계산된 3개의 정확도를 평균 낸다.

  - 의의
    1. 3번 정확도를 계산해 평균 낸 것이므로, 1번 정확도를 계산하는 것보다 신뢰성이 높다.
    2. 위와 같은 방법을 3th fold Cross Validation이라고 부른다.
    3. 5번 나누어서 위와 같이 하면 5th fold Cross Validation이라고 부른다.
    4. 몇번 나눌지는 자유이다.
